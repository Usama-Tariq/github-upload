{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Breast Cancer Wisconsin (Original) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29)\n",
    "\n",
    "Samples arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore reflects this chronological grouping of the data. This grouping information appears immediately below, having been removed from the data itself:\n",
    "\n",
    "- Group 1: 367 instances (January 1989)\n",
    "- Group 2:  70 instances (October 1989)\n",
    "- Group 3:  31 instances (February 1990)\n",
    "- Group 4:  17 instances (April 1990)\n",
    "- Group 5:  48 instances (August 1990)\n",
    "- Group 6:  49 instances (Updated January 1991)\n",
    "- Group 7:  31 instances (June 1991)\n",
    "- Group 8:  86 instances (November 1991)\n",
    "#### ----------------------------------------------------------------------------------------\n",
    "Total:   699 points (as of the donated datbase on 15 July 1992)\n",
    "\n",
    "Note that the results summarized above in Past Usage refer to a dataset of size 369, while Group 1 has only 367 instances.  This is because it originally contained 369 instances; 2 were removed.  The following statements summarizes changes to the original Group 1's set of data:\n",
    "\n",
    "   -   Group 1 : 367 points: 200B 167M (January 1989)\n",
    "   -   Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
    "   -   Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
    "   -  Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
    "   -  Changed 0 to 1 in field 6 of sample 1219406\n",
    "   -  Changed 0 to 1 in field 8 of following sample:\n",
    "   -  1182404,2,3,1,1,1,2,0,1,1,1\n",
    "\n",
    "The data can be accessed at [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data), and its description is given [here](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names).\n",
    "### Number of Instances: 699\n",
    "### Number of Attributes: 10 plus the class attribute\n",
    "\n",
    "Attribute Information: (class attribute has been moved to last column)\n",
    "\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "   9. Mitoses                       1 - 10\n",
    "   9. Class:                        \n",
    "  10. 2 for benign \n",
    "  11. 4 for malignant\n",
    "  \n",
    "- Missing attribute values:\n",
    "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
    "   (i.e., unavailable) attribute value, now denoted by \"?\".\n",
    "- Class distribution:\n",
    "    - Benign: 458 (65.5%)\n",
    "    - Malignant: 241 (34.5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Members\n",
    "- #### Usama Saeed [BSEF14M547]\n",
    "- #### H. Usama Tariq [BSEF14M556]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Reading data from CSV file into a DataFrame from link\n",
    "data_frame = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', \n",
    "                 sep=',', header=None ) \n",
    "\n",
    "# Printing data for own convenience \n",
    "# pd.options.display.max_rows=700\n",
    "# pd.options.display.max_columns=15\n",
    "# print data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Read data into another DataFrame\n",
    "data_frame_1 = data_frame\n",
    "\n",
    "# Column names of data and assigning those name to data\n",
    "columns = [\"Sample Code Number\", \"Clump Thickness\", \"Uniformity of Cell Size\", \n",
    "       \"Uniformity of Cell Shape\", \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \n",
    "        \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"Class\"]\n",
    "#df1 = df1.rename(columns=col) \n",
    "data_frame_1.columns = columns\n",
    "\n",
    "# Printing data for own convenience \n",
    "# pd.options.display.max_rows=700\n",
    "# pd.options.display.max_columns=15\n",
    "# print data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Cleaning the Data\n",
    "data_frame_1 = data_frame_1.replace('?', np.nan)\n",
    "\n",
    "# Replacing All Missing Numeric Values with Mode  \n",
    "imputer = Imputer(missing_values=np.nan, strategy='most_frequent', axis=0)\n",
    "for i in list(data_frame_1):\n",
    "    data_frame_1[[i]]=imputer.fit_transform(data_frame_1[[i]])\n",
    "\n",
    "# Separating Target 'Y' from Data\n",
    "Y = data_frame_1['Class']\n",
    "del data_frame_1['Class']\n",
    "Y = [0 if i==2 else 1 for i in Y ]\n",
    "\n",
    "# Printing data for own convenience \n",
    "# pd.options.display.max_rows=700\n",
    "# pd.options.display.max_columns=15\n",
    "# print data_frame_1\n",
    "# print Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Scaling the Data\n",
    "scalar = MinMaxScaler()\n",
    "data_frame_1_scale = scalar.fit_transform(data_frame_1)\n",
    "\n",
    "# Cross-Validation folds\n",
    "cv = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def KNeighbors_Classifier(x, y):\n",
    "    params = dict(n_neighbors = list(range(1,31)))\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), params, n_jobs=-1, cv=cv)\n",
    "    grid.fit(x,y)\n",
    "    return (grid.best_score_,grid.best_params_)\n",
    "\n",
    "def Gaussian_NB(x, y):\n",
    "    params = dict(priors = [None])\n",
    "    grid = GridSearchCV(GaussianNB(), params, n_jobs=-1, cv=cv)\n",
    "    grid.fit(x,y)\n",
    "    return (grid.best_score_,grid.best_params_)\n",
    "\n",
    "def DecisionTree_Classifier(x, y, nf):\n",
    "    params = dict(max_depth= list(range(1,nf)), max_features= [nf])\n",
    "    grid = GridSearchCV(DecisionTreeClassifier(), params, n_jobs=-1, cv=cv)\n",
    "    grid.fit(x,y)\n",
    "    return (grid.best_score_,grid.best_params_)\n",
    "\n",
    "def RandomForest_Classifier(x, y, nf):\n",
    "    params = dict(max_depth= list(range(1,nf)), max_features= [nf], n_estimators=[nf])\n",
    "    grid = GridSearchCV(RandomForestClassifier(), params, n_jobs=-1, cv=cv)\n",
    "    grid.fit(x,y)\n",
    "    return (grid.best_score_,grid.best_params_)\n",
    "        \n",
    "def SupportVector_Classifier(x, y):\n",
    "    params = dict(C = [0.001, 0.01, 0.1, 1, 10, 15, 20, 25], gamma= [0.001, 0.01, 0.1, 1])\n",
    "    grid = GridSearchCV(SVC(), params, n_jobs=-1, cv=cv)\n",
    "    grid.fit(x,y)\n",
    "    return (grid.best_score_,grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighbors_Classifier_score, KNeighbors_Classifier_params = KNeighbors_Classifier(data_frame_1_scale, Y)\n",
    "Gaussian_NB_score, Gaussian_NB_params = Gaussian_NB(data_frame_1_scale, Y)\n",
    "DecisionTree_Classifier_score, DecisionTree_Classifier_params = DecisionTree_Classifier(data_frame_1_scale, \n",
    "                                                                                        Y, data_frame_1_scale.shape[1])\n",
    "RandomForest_Classifier_score, RandomForest_Classifier_params = RandomForest_Classifier(data_frame_1_scale, \n",
    "                                                                                        Y, data_frame_1_scale.shape[1])\n",
    "SupportVector_Classifier_score, SupportVector_Classifier_params = SupportVector_Classifier(data_frame_1_scale, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier Score :  0.9699570815450643\n",
      "KNeighbors Classifier Params:  {'n_neighbors': 8}\n",
      "-------------------------------------------------------\n",
      "Gaussian Naive Bayes Score :  0.9599427753934192\n",
      "Gaussian Naive Bayes Params :  {'priors': None}\n",
      "-------------------------------------------------------\n",
      "Decision Tree Classifier Score :  0.9456366237482118\n",
      "Decision Tree Classifier Params :  {'max_features': 10L, 'max_depth': 3}\n",
      "-------------------------------------------------------\n",
      "Random Forest Classifier Score :  0.9585121602288984\n",
      "Random Forest Classifier Params :  {'max_features': 10L, 'n_estimators': 10L, 'max_depth': 9}\n",
      "-------------------------------------------------------\n",
      "Support Vector Classifier Score :  0.9670958512160229\n",
      "Support Vector Classifier Params :  {'C': 0.1, 'gamma': 1}\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print \"KNeighbors Classifier Score : \", KNeighbors_Classifier_score\n",
    "print \"KNeighbors Classifier Params: \", KNeighbors_Classifier_params\n",
    "print \"-------------------------------------------------------\"\n",
    "print \"Gaussian Naive Bayes Score : \", Gaussian_NB_score\n",
    "print \"Gaussian Naive Bayes Params : \", Gaussian_NB_params\n",
    "print \"-------------------------------------------------------\"\n",
    "print \"Decision Tree Classifier Score : \", DecisionTree_Classifier_score\n",
    "print \"Decision Tree Classifier Params : \", DecisionTree_Classifier_params\n",
    "print \"-------------------------------------------------------\"\n",
    "print \"Random Forest Classifier Score : \", RandomForest_Classifier_score\n",
    "print \"Random Forest Classifier Params : \", RandomForest_Classifier_params\n",
    "print \"-------------------------------------------------------\"\n",
    "print \"Support Vector Classifier Score : \", SupportVector_Classifier_score\n",
    "print \"Support Vector Classifier Params : \", SupportVector_Classifier_params\n",
    "print \"-------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bc\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\bc\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "data_frame_1_poly = PolynomialFeatures(degree = 2).fit(data_frame_1_scale).transform(data_frame_1_scale)\n",
    "#print data_frame_1_poly.shape\n",
    "\n",
    "no_features = range(10, 100, 15)\n",
    "\n",
    "best_accuracy = []\n",
    "\n",
    "\n",
    "for i in no_features:\n",
    "    accuracy = 0\n",
    "    data_frame_1_select = SelectPercentile(percentile=i).fit(data_frame_1_poly,Y).transform(data_frame_1_poly)\n",
    "    #print data_frame_1_select.shape[1]\n",
    "    temp = KNeighbors_Classifier(data_frame_1_select, Y)\n",
    "    if temp[0] > accuracy:\n",
    "        accuracy_temp = (\"KNeighbors_Classifier\",data_frame_1_select.shape[1], temp[0], temp[1])\n",
    "        accuracy = temp[0]\n",
    "    \n",
    "    temp = Gaussian_NB(data_frame_1_select, Y)\n",
    "    if temp[0] > accuracy:\n",
    "        accuracy_temp = (\"Gaussian_NB\",data_frame_1_select.shape[1], temp[0], temp[1])\n",
    "        accuracy = temp[0]\n",
    "    \n",
    "    temp = DecisionTree_Classifier(data_frame_1_select, Y, data_frame_1_select.shape[1])\n",
    "    if temp[0] > accuracy:\n",
    "        accuracy_temp = (\"DecisionTree_Classifier\",data_frame_1_select.shape[1], temp[0], temp[1])\n",
    "        accuracy = temp[0]\n",
    "    \n",
    "    temp = RandomForest_Classifier(data_frame_1_select, Y, data_frame_1_select.shape[1])\n",
    "    if temp[0] > accuracy:\n",
    "        accuracy_temp = (\"RandomForest_Classifier\",data_frame_1_select.shape[1], temp[0], temp[1])\n",
    "        accuracy = temp[0]\n",
    "    \n",
    "    temp = SupportVector_Classifier(data_frame_1_select, Y)\n",
    "    if temp[0] > accuracy:\n",
    "        accuracy_temp = (\"SupportVector_Classifier\",data_frame_1_select.shape[1], temp[0], temp[1])\n",
    "        accuracy = temp[0]\n",
    "        \n",
    "    best_accuracy.append(accuracy_temp)\n",
    "        \n",
    "from operator import itemgetter\n",
    "\n",
    "best_accuracy = sorted(best_accuracy, key=itemgetter(2), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'KNeighbors_Classifier', with selected features '36', and params '{'n_neighbors': 5}' provides Best Accuracy: 0.971388\n",
      "'KNeighbors_Classifier', with selected features '46', and params '{'n_neighbors': 3}' provides Best Accuracy: 0.971388\n",
      "'KNeighbors_Classifier', with selected features '17', and params '{'n_neighbors': 7}' provides Best Accuracy: 0.969957\n",
      "'SupportVector_Classifier', with selected features '26', and params '{'C': 0.01, 'gamma': 1}' provides Best Accuracy: 0.969957\n",
      "'KNeighbors_Classifier', with selected features '56', and params '{'n_neighbors': 3}' provides Best Accuracy: 0.968526\n",
      "'Gaussian_NB', with selected features '7', and params '{'priors': None}' provides Best Accuracy: 0.964235\n"
     ]
    }
   ],
   "source": [
    "for i in best_accuracy:\n",
    "    print (\"'%s', with selected features '%d', and params '%s' provides Best Accuracy: %f\" % (i[0], i[1], i[3], i[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors_Classifier  with selected features ' 36 ', and params ' {'n_neighbors': 5} ', provides Accuracy :' 0.9713876967095851 ' \n",
      "\n",
      "KNeighbors_Classifier  with selected features ' 46 ', and params ' {'n_neighbors': 3} ', provides Accuracy :' 0.9713876967095851 ' \n",
      "\n",
      "KNeighbors_Classifier  with selected features ' 17 ', and params ' {'n_neighbors': 7} ', provides Accuracy :' 0.9699570815450643 ' \n",
      "\n",
      "SupportVector_Classifier  with selected features ' 26 ', and params ' {'C': 0.01, 'gamma': 1} ', provides Accuracy :' 0.9699570815450643 ' \n",
      "\n",
      "KNeighbors_Classifier  with selected features ' 56 ', and params ' {'n_neighbors': 3} ', provides Accuracy :' 0.9685264663805436 ' \n",
      "\n",
      "Gaussian_NB  with selected features ' 7 ', and params ' {'priors': None} ', provides Accuracy :' 0.9642346208869814 ' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in best_accuracy:\n",
    "    print i[0], \" with selected features '\", i[1], \"', and params '\", i[3], \"', provides Accuracy :'\", i[2], \"'\", '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best Accuracy on the data with original features (i.e 10 in number) is [\"96.99%\"]()while after performing Feature Engineering (Degree=2) the Best Accuracy is [\"97.13%\"]() with '36' features. Though after performing the process of feature selection, accuracy increased just [\"0.14%\"]() i.e a very minimal difference. This shows that our model is already performing good results without feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
